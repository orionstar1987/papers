<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>Category</th>
      <th>Paper Name</th>
      <th>Description</th>
      <th>Citation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2018.9</td>
      <td>SARs</td>
      <td><a href="https://www.ijcai.org/Proceedings/2019/0883.pdf">Sequential Recommender Systems: Challenges, Progress and Prospects</a></td>
      <td>
        <ul>
          <li>Capture the user-item interactinos as a dynamic sequence (vs. static with traditional methods) and take the sequential dependencies into account to capture the current and recent preference of a user for more accurate recommendation</li>
          <li>SRS takes a sequence of user-item interactions as the input and tries to predict the subsequent user-item interactions that may happen in the near future through modelling the complex sequential dependencies embedded in the sequence of user-item interactions</li>  
          <li>Benefit 1: capure user behavior/preference change over time</li>  
          <li>Benefit 2: user-item interactions are context based. Diversify recommendations by avoiding repeatedly recommending thos items identical or simlar to those already chosen</li>
        </ul>
      </td>
      <td><a href="https://www.semanticscholar.org/paper/Sequential-Recommender-Systems%3A-Challenges%2C-and-Wang-Hu/d837642802ffc85e193694f94f7499276864648e"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd837642802ffc85e193694f94f7499276864648e%3Ffields%3DcitationCount" alt="citation"></a></td>
    </tr>
    <tr>
      <td>2018.8</td>
      <td>SARs</td>
      <td><a href="https://arxiv.org/pdf/1808.09781.pdf">Self-Attentive Sequential Recommendation</a></td>
      <td>
        <ul>
          <li>Balance traditional methods such as Markov Chains (makes its predictions based on relatively few actions) and RNN (capture long-term semantics)</li>
          <li></li>  
          <li></li>  
          <li></li>
        </ul>
      </td>
      <td><a href="https://www.semanticscholar.org/paper/Sequential-Recommender-Systems%3A-Challenges%2C-and-Wang-Hu/d837642802ffc85e193694f94f7499276864648e"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd837642802ffc85e193694f94f7499276864648e%3Ffields%3DcitationCount" alt="citation"></a></td>
    </tr>
    <tr>
      <td>2014.9</td>
      <td>seq2seq</td>
      <td><a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a></td>
      <td>
        <ul>
          <li>machine translation: can handle variable input and variable output lengths</li>
          <li>decouple Encoder and Decoder </li>
          <li>use Embeddings as input </li>
          <li>stacked LSTM cells and layers for both Encoder and Decoder</li>
          <li>Teacher Forcing: during training, use the known/actual words and stop at the known phrase length, rather than using the predicted tokens</li>
        </ul>
      </td>
      <td><a href="https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcea967b59209c6be22829699f05b8b1ac4dc092d%3Ffields%3DcitationCount" alt="citation"></a></td>
    </tr>
