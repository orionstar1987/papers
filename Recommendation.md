<table>
  <thead>
    <tr>
      <th>Date</th>
      <th>Category</th>
      <th>Paper Name</th>
      <th>Description</th>
      <th>Citation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td></td>
      <td>SARs</td>
      <td><a href="https://www.ijcai.org/Proceedings/2019/0883.pdf">Sequential Recommender Systems: Challenges, Progress and Prospects</a></td>
      <td>Sequential Recommender Systems: Challenges, Progress and Prospects</td>
      <td><a href="https://www.semanticscholar.org/paper/Sequential-Recommender-Systems%3A-Challenges%2C-and-Wang-Hu/d837642802ffc85e193694f94f7499276864648e"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fd837642802ffc85e193694f94f7499276864648e%3Ffields%3DcitationCount" alt="citation"></a></td>
    </tr>
    <tr>
      <td></td>
      <td>LSTM</td>
      <td></td>
      <td>
        <ul>
          <li>2 separate paths (long term memories and short term memories)</li>
          <li>3 gates (forget, input, output)</li>
        </ul>
      </td>
      <td><a href="https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F204e3073870fae3d05bcbc2f6a8e263d9b72e776%3Ffields%3DcitationCount" alt="citation"></a></td>
    </tr>
    <tr>
      <td>2014.9</td>
      <td>seq2seq</td>
      <td><a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a></td>
      <td>
        <ul>
          <li>machine translation: can handle variable input and variable output lengths</li>
          <li>decouple Encoder and Decoder </li>
          <li>use Embeddings as input </li>
          <li>stacked LSTM cells and layers for both Encoder and Decoder</li>
          <li>Teacher Forcing: during training, use the known/actual words and stop at the known phrase length, rather than using the predicted tokens</li>
        </ul>
      </td>
      <td><a href="https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d"><img src="https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2Fcea967b59209c6be22829699f05b8b1ac4dc092d%3Ffields%3DcitationCount" alt="citation"></a></td>
    </tr>
